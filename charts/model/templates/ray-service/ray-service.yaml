{{- $modelRepository := .Values.persistence.persistentVolumeClaim.modelRepository -}}
{{- $rayConda := .Values.persistence.persistentVolumeClaim.rayConda -}}
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: {{ template "model.ray-service" . }}
  # annotations:
  #   ray.io/ft-enabled: "true"
spec:
  rayVersion: {{ .Values.rayService.image.version }}
  ## raycluster autoscaling config
  enableInTreeAutoscaling: true
  autoscalerOptions:
    upscalingMode: Default
    # idleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.
    idleTimeoutSeconds: 60
    imagePullPolicy: IfNotPresent
    securityContext: {}
    env: []
    envFrom: []
    resources:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "500m"
        memory: "512Mi"
  headGroupSpec:
    rayStartParams:
      num-cpus: "0"
      disable-usage-stats: "true"
    template:
      spec:
        containers:
          - name: ray-head
            image: {{ .Values.rayService.image.repository }}:{{ .Values.rayService.image.tag }}
            resources:
              limits:
                cpu: "2"
                memory: "4Gi"
              requests:
                cpu: "2"
                memory: "4Gi"
            # env:
            #   - name: RAY_REDIS_ADDRESS
            #     {{ template "core.redis.addr" . }}
            ports:
              - containerPort: 6379
                name: gcs-server
              - containerPort: 8265
                name: dashboard
              - containerPort: 10001
                name: client
              - containerPort: 8000
                name: serve
              - containerPort: 9000
                name: serve-grpc
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh","-c","ray stop"]
  workerGroupSpecs:
    - replicas: 1
      minReplicas: 1
      maxReplicas: 5
      groupName: cpu-group
      rayStartParams:
          disable-usage-stats: "true"
      #pod template
      template:
        spec:
          volumes:
            - name: ray-conda
            {{- if not .Values.persistence.enabled }}
              emptyDir: {}
            {{- else if $rayConda.existingClaim }}
              persistentVolumeClaim:
                claimName: {{ $rayConda.existingClaim }}
            {{- else }}
              persistentVolumeClaim:
                claimName: ray-conda-data-volume
            {{- end }}
            - name: model-repository
            {{- if not .Values.persistence.enabled }}
              emptyDir: {}
            {{- else if $modelRepository.existingClaim }}
              persistentVolumeClaim:
                claimName: {{ $modelRepository.existingClaim }}
            {{- else }}
              persistentVolumeClaim:
                claimName: model-repository-data-volume
            {{- end }}
            - name: cp-conda-env-configmap
              configMap:
                name: cp-conda-env
                defaultMode: 0777
                items:
                  - key: cp_conda_env_and_start_ray_serve.sh
                    path: cp_conda_env_and_start_ray_serve.sh
          containers:
            - name: ray-worker
              image: {{ .Values.rayService.image.repository }}:{{ .Values.rayService.image.tag }}
              lifecycle:
                postStart:
                  exec:
                    command: ["/bin/sh","-c","/home/ray/script/cp_conda_env_and_start_ray_serve.sh"]
                preStop:
                  exec:
                    command: ["/bin/sh","-c","ray stop"]
              # TODO: determine how big the head node should be
              # Optimal resource allocation will depend on our Kubernetes infrastructure and might
              # require some experimentation.
              # Setting requests=limits is recommended with Ray. K8s limits are used for Ray-internal
              # resource accounting. K8s requests are not used by Ray.
              # this also apply to the workerGroup
              resources:
                limits:
                  cpu: "2"
                  memory: "4Gi"
                requests:
                  cpu: "2"
                  memory: "4Gi"
              volumeMounts:
                - mountPath: /ray-conda-pack
                  name: ray-conda
                - mountPath: /model-repository
                  name: model-repository
                - mountPath: /home/ray/script
                  name: cp-conda-env-configmap
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cp-conda-env
data:
  cp_conda_env_and_start_ray_serve.sh: |
    #!/bin/bash

    # wait for ray cluster to finish initialization
    while true; do
        ray health-check 2>/dev/null
        if [ "$?" = "0" ]; then
            break
        else
            echo "INFO: waiting for ray head to start"
            sleep 1
        fi
    done

    cp -r /home/ray/anaconda3/* /ray-conda-pack

    echo "INFO: Conda env copying done"

    serve start --http-host=0.0.0.0 --grpc-port 9000 --grpc-servicer-functions ray_pb2_grpc.add_RayServiceServicer_to_server

    echo "INFO: Start ray serve"
